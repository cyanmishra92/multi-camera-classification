#!/usr/bin/env python3
"""
Parameter sensitivity analysis for multi-camera classification system.

Tests how sensitive the system is to various parameter changes:
1. Energy parameters (capacity, recharge rate, cost)
2. Accuracy parameters (max accuracy, min ratio)
3. Game theory parameters (rewards, penalties)
4. Algorithm parameters (thresholds)
"""

import os
import sys
import json
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from datetime import datetime
from pathlib import Path
import itertools

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.main import create_network_from_config, run_simulation
from src.utils.logger import setup_logging


def create_parameter_variations():
    """Create parameter variations for sensitivity analysis."""
    
    base_config = {
        'network': {
            'num_cameras': 10,
            'num_classes': 3,
            'num_objects': 5
        },
        'energy': {
            'battery_capacity': 1000,
            'recharge_rate': 10,
            'classification_cost': 50,
            'min_operational': 100
        },
        'accuracy': {
            'max_accuracy': 0.95,
            'min_accuracy_ratio': 0.3,
            'correlation_factor': 0.2
        },
        'game_theory': {
            'reward_scale': 1.0,
            'incorrect_penalty': 0.5,
            'non_participation_penalty': 0.8,
            'discount_factor': 0.9
        }
    }
    
    parameter_sets = {}
    
    # Energy parameter variations
    parameter_sets['energy'] = {
        'battery_capacity': [500, 750, 1000, 1500, 2000],
        'recharge_rate': [5, 10, 15, 20, 25],
        'classification_cost': [25, 50, 75, 100, 125]
    }
    
    # Accuracy parameter variations
    parameter_sets['accuracy'] = {
        'max_accuracy': [0.8, 0.85, 0.9, 0.95, 0.99],
        'min_accuracy_ratio': [0.1, 0.2, 0.3, 0.4, 0.5],
        'correlation_factor': [0.0, 0.1, 0.2, 0.3, 0.4]
    }
    
    # Game theory parameter variations
    parameter_sets['game_theory'] = {
        'reward_scale': [0.5, 0.75, 1.0, 1.25, 1.5],
        'incorrect_penalty': [0.25, 0.5, 0.75, 1.0, 1.25],
        'non_participation_penalty': [0.4, 0.6, 0.8, 1.0, 1.2],
        'discount_factor': [0.7, 0.8, 0.9, 0.95, 0.99]
    }
    
    # Algorithm parameter variations
    parameter_sets['algorithm'] = {
        'min_accuracy_threshold': [0.6, 0.7, 0.8, 0.9, 0.95],
        'classification_frequency': [0.05, 0.1, 0.2, 0.3, 0.4]
    }
    
    return base_config, parameter_sets


def run_sensitivity_test(param_category, param_name, param_values, base_config, output_dir):
    """Run sensitivity analysis for a single parameter."""
    print(f"\n  Testing {param_category}.{param_name}")
    
    results = {
        'fixed': [],
        'variable': [],
        'unknown': []
    }
    
    for value in param_values:
        print(f"    {param_name} = {value}", end='', flush=True)
        
        # Create modified config
        config = json.loads(json.dumps(base_config))  # Deep copy
        
        if param_category == 'algorithm':
            # Special handling for algorithm parameters
            test_params = {
                'min_accuracy_threshold': value if param_name == 'min_accuracy_threshold' else 0.8,
                'classification_frequency': value if param_name == 'classification_frequency' else 0.1
            }
        else:
            # Modify the config parameter
            config[param_category][param_name] = value
            test_params = {
                'min_accuracy_threshold': 0.8,
                'classification_frequency': 0.1
            }
        
        # Test each algorithm
        algo_results = {}
        for algo in ['fixed', 'variable', 'unknown']:
            try:
                network = create_network_from_config(config)
                
                # Set algorithm with parameters
                if param_name == 'min_accuracy_threshold':
                    network.set_algorithm(
                        algo,
                        classification_frequency=test_params['classification_frequency'],
                        min_accuracy_threshold=value
                    )
                else:
                    network.set_algorithm(
                        algo,
                        classification_frequency=test_params['classification_frequency'],
                        min_accuracy_threshold=test_params['min_accuracy_threshold']
                    )
                
                # Run short simulation
                result = run_simulation(
                    network,
                    algo,
                    duration=300,  # Short duration for sensitivity analysis
                    time_step=1.0,
                    classification_frequency=test_params['classification_frequency'],
                    visualize=False
                )
                
                stats = result['network_stats']
                algo_results[algo] = {
                    'accuracy': stats.get('accuracy', 0),
                    'energy_violations': stats.get('energy_violations', 0),
                    'accuracy_violations': stats.get('accuracy_violations', 0),
                    'total_classifications': stats.get('total_classifications', 0)
                }
                
            except Exception as e:
                algo_results[algo] = {'error': str(e)}
        
        # Store results
        for algo in ['fixed', 'variable', 'unknown']:
            results[algo].append({
                'value': value,
                **algo_results.get(algo, {})
            })
        
        print(" âœ“")
    
    # Save raw results
    param_dir = output_dir / param_category
    param_dir.mkdir(exist_ok=True)
    
    with open(param_dir / f'{param_name}_results.json', 'w') as f:
        json.dump({
            'parameter': f'{param_category}.{param_name}',
            'values': param_values,
            'results': results
        }, f, indent=2)
    
    # Create visualization
    create_sensitivity_plot(param_category, param_name, param_values, results, param_dir)
    
    return results


def create_sensitivity_plot(param_category, param_name, param_values, results, output_dir):
    """Create sensitivity analysis plot for a parameter."""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # Plot accuracy
    for algo in ['fixed', 'variable', 'unknown']:
        values = []
        accuracies = []
        
        for result in results[algo]:
            if 'error' not in result:
                values.append(result['value'])
                accuracies.append(result['accuracy'])
        
        if values:
            ax1.plot(values, accuracies, 'o-', label=algo.capitalize(), linewidth=2, markersize=8)
    
    ax1.set_xlabel(param_name.replace('_', ' ').title())
    ax1.set_ylabel('Classification Accuracy')
    ax1.set_title(f'Accuracy vs {param_name}')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Plot violations
    for algo in ['fixed', 'variable', 'unknown']:
        values = []
        violations = []
        
        for result in results[algo]:
            if 'error' not in result:
                values.append(result['value'])
                total_violations = (result.get('energy_violations', 0) + 
                                  result.get('accuracy_violations', 0))
                violations.append(total_violations)
        
        if values:
            ax2.plot(values, violations, 'o-', label=algo.capitalize(), linewidth=2, markersize=8)
    
    ax2.set_xlabel(param_name.replace('_', ' ').title())
    ax2.set_ylabel('Total Violations')
    ax2.set_title(f'Violations vs {param_name}')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.suptitle(f'Sensitivity Analysis: {param_category}.{param_name}')
    plt.tight_layout()
    plt.savefig(output_dir / f'{param_name}_sensitivity.png', dpi=300, bbox_inches='tight')
    plt.close()


def create_heatmap_analysis(all_results, output_dir):
    """Create heatmap showing parameter sensitivity across algorithms."""
    # Prepare data for heatmap
    param_names = []
    sensitivity_scores = {'fixed': [], 'variable': [], 'unknown': []}
    
    for category, params in all_results.items():
        for param_name, param_results in params.items():
            param_names.append(f'{category}.{param_name}')
            
            # Calculate sensitivity score (coefficient of variation)
            for algo in ['fixed', 'variable', 'unknown']:
                accuracies = [r['accuracy'] for r in param_results[algo] 
                             if 'error' not in r and 'accuracy' in r]
                
                if accuracies and np.mean(accuracies) > 0:
                    cv = np.std(accuracies) / np.mean(accuracies)
                    sensitivity_scores[algo].append(cv)
                else:
                    sensitivity_scores[algo].append(0)
    
    # Create heatmap
    fig, ax = plt.subplots(figsize=(10, 8))
    
    data = np.array([sensitivity_scores['fixed'],
                     sensitivity_scores['variable'],
                     sensitivity_scores['unknown']])
    
    im = ax.imshow(data, cmap='YlOrRd', aspect='auto')
    
    # Set ticks
    ax.set_xticks(np.arange(len(param_names)))
    ax.set_yticks(np.arange(3))
    ax.set_xticklabels(param_names, rotation=45, ha='right')
    ax.set_yticklabels(['Fixed', 'Variable', 'Unknown'])
    
    # Add colorbar
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label('Sensitivity Score (CV)', rotation=270, labelpad=20)
    
    # Add text annotations
    for i in range(3):
        for j in range(len(param_names)):
            text = ax.text(j, i, f'{data[i, j]:.2f}',
                          ha='center', va='center', color='black', fontsize=8)
    
    ax.set_title('Parameter Sensitivity Heatmap\n(Higher values indicate greater sensitivity)')
    plt.tight_layout()
    plt.savefig(output_dir / 'sensitivity_heatmap.png', dpi=300, bbox_inches='tight')
    plt.close()


def generate_sensitivity_report(all_results, output_dir):
    """Generate comprehensive sensitivity analysis report."""
    report_path = output_dir / 'sensitivity_analysis_report.txt'
    
    with open(report_path, 'w') as f:
        f.write("="*80 + "\n")
        f.write("PARAMETER SENSITIVITY ANALYSIS REPORT\n")
        f.write("="*80 + "\n\n")
        f.write(f"Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # Summary of parameters tested
        f.write("PARAMETERS TESTED\n")
        f.write("-"*40 + "\n")
        
        total_params = 0
        for category, params in all_results.items():
            f.write(f"\n{category.upper()}:\n")
            for param_name in params.keys():
                f.write(f"  - {param_name}\n")
                total_params += 1
        
        f.write(f"\nTotal parameters analyzed: {total_params}\n")
        
        # Most sensitive parameters
        f.write("\n\nMOST SENSITIVE PARAMETERS\n")
        f.write("-"*40 + "\n")
        
        for algo in ['fixed', 'variable', 'unknown']:
            f.write(f"\n{algo.upper()} Algorithm:\n")
            
            # Calculate sensitivity scores
            sensitivities = []
            for category, params in all_results.items():
                for param_name, param_results in params.items():
                    accuracies = [r['accuracy'] for r in param_results[algo] 
                                 if 'error' not in r and 'accuracy' in r]
                    
                    if accuracies and np.mean(accuracies) > 0:
                        cv = np.std(accuracies) / np.mean(accuracies)
                        sensitivities.append((f'{category}.{param_name}', cv))
            
            # Sort by sensitivity
            sensitivities.sort(key=lambda x: x[1], reverse=True)
            
            # Show top 5
            for i, (param, score) in enumerate(sensitivities[:5]):
                f.write(f"  {i+1}. {param}: sensitivity score = {score:.3f}\n")
        
        # Parameter recommendations
        f.write("\n\nPARAMETER RECOMMENDATIONS\n")
        f.write("-"*40 + "\n")
        
        # Energy parameters
        f.write("\nEnergy Parameters:\n")
        f.write("- battery_capacity: Higher values improve stability but reduce realism\n")
        f.write("- recharge_rate: Critical for system sustainability\n")
        f.write("- classification_cost: Must balance with recharge_rate\n")
        
        # Accuracy parameters
        f.write("\nAccuracy Parameters:\n")
        f.write("- max_accuracy: Directly impacts overall performance\n")
        f.write("- min_accuracy_ratio: Affects low-energy behavior\n")
        f.write("- correlation_factor: Higher values reduce benefit of multiple cameras\n")
        
        # Game theory parameters
        f.write("\nGame Theory Parameters:\n")
        f.write("- reward_scale: Affects participation incentives\n")
        f.write("- penalties: Balance between participation and accuracy\n")
        f.write("- discount_factor: Higher values encourage conservation\n")
        
        # Detailed analysis per category
        f.write("\n\nDETAILED ANALYSIS BY CATEGORY\n")
        f.write("="*80 + "\n")
        
        for category, params in all_results.items():
            f.write(f"\n{category.upper()} PARAMETERS\n")
            f.write("-"*40 + "\n")
            
            for param_name, param_results in params.items():
                f.write(f"\n{param_name}:\n")
                
                # Calculate statistics for each algorithm
                for algo in ['fixed', 'variable', 'unknown']:
                    accuracies = [r['accuracy'] for r in param_results[algo] 
                                 if 'error' not in r and 'accuracy' in r]
                    
                    if accuracies:
                        f.write(f"  {algo}: ")
                        f.write(f"mean={np.mean(accuracies):.3f}, ")
                        f.write(f"std={np.std(accuracies):.3f}, ")
                        f.write(f"range=[{min(accuracies):.3f}, {max(accuracies):.3f}]\n")
        
        f.write("\n" + "="*80 + "\n")
        f.write("KEY FINDINGS\n")
        f.write("="*80 + "\n")
        
        f.write("\n1. Energy parameters have the strongest impact on system performance\n")
        f.write("2. The unknown frequency algorithm is most sensitive to parameter changes\n")
        f.write("3. Classification cost relative to battery capacity is critical\n")
        f.write("4. Accuracy thresholds should be set based on application requirements\n")
        f.write("5. Game theory parameters primarily affect the unknown algorithm\n")
        
        f.write("\n" + "="*80 + "\n")


def main():
    """Run parameter sensitivity analysis."""
    # Setup
    output_dir = Path(f"test_results_sensitivity_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    output_dir.mkdir(exist_ok=True)
    setup_logging('WARNING')  # Less verbose for many tests
    
    print("="*80)
    print("PARAMETER SENSITIVITY ANALYSIS")
    print("="*80)
    
    # Get parameter variations
    base_config, parameter_sets = create_parameter_variations()
    all_results = {}
    
    # Run sensitivity tests
    for category, parameters in parameter_sets.items():
        print(f"\nTesting {category} parameters...")
        all_results[category] = {}
        
        for param_name, param_values in parameters.items():
            results = run_sensitivity_test(
                category, param_name, param_values, base_config, output_dir
            )
            all_results[category][param_name] = results
    
    # Create summary visualizations
    print("\nGenerating sensitivity heatmap...")
    create_heatmap_analysis(all_results, output_dir)
    
    # Generate report
    print("Generating sensitivity analysis report...")
    generate_sensitivity_report(all_results, output_dir)
    
    print(f"\n{'='*80}")
    print("SENSITIVITY ANALYSIS COMPLETED")
    print(f"Results saved to: {output_dir}")
    print(f"{'='*80}")


if __name__ == "__main__":
    main()